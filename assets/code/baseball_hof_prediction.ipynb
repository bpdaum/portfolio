{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Predicting Hall of Fame Inductees using FanGraphs Data (Decade Loop)\n",
                "\n",
                "## Objective\n",
                "Build an XGBoost classifier to predict Baseball Hall of Fame induction using **FanGraphs** data via `pybaseball`. \n",
                "\n",
                "**Note:** To avoid server-side errors (HTTP 500) from requesting too much data at once, we fetch batting statistics in **decade-long chunks** (e.g., 1970-1979, 1980-1989) and aggregate them.\n",
                "\n",
                "## Tech Stack\n",
                "*   **Python & Pandas**: Data processing\n",
                "*   **Pybaseball**: Access to FanGraphs data\n",
                "*   **XGBoost**: Classification model\n",
                "*   **Scikit-Learn**: Metrics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install libraries\n",
                "!pip install pybaseball xgboost scikit-learn matplotlib seaborn pandas numpy"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import xgboost as xgb\n",
                "import pybaseball\n",
                "from pybaseball import batting_stats\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import time\n",
                "\n",
                "plt.style.use('fivethirtyeight')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Data Loading (Loop by Decade)\n",
                "We fetch data from 1970 to 2025 in 10-year increments."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def fetch_data_by_decade(start_year, end_year):\n",
                "    all_data = []\n",
                "    \n",
                "    # Iterate through decades\n",
                "    for year in range(start_year, end_year + 1, 10):\n",
                "        # Define the window (e.g., 1970-1979)\n",
                "        decade_start = year\n",
                "        decade_end = min(year + 9, end_year)\n",
                "        \n",
                "        print(f\"Fetching data for {decade_start}-{decade_end}...\")\n",
                "        \n",
                "        try:\n",
                "            # Fetch stats for this chunk\n",
                "            # qual=0 ensures we get all players, not just qualified ones\n",
                "            df_chunk = batting_stats(decade_start, decade_end, qual=0)\n",
                "            all_data.append(df_chunk)\n",
                "            \n",
                "            # Be polite to the server\n",
                "            time.sleep(2)\n",
                "            \n",
                "        except Exception as e:\n",
                "            print(f\"Error fetching {decade_start}-{decade_end}: {e}\")\n",
                "            continue\n",
                "            \n",
                "    if not all_data:\n",
                "        raise ValueError(\"No data could be fetched!\")\n",
                "        \n",
                "    # Combine all chunks\n",
                "    return pd.concat(all_data, ignore_index=True)\n",
                "\n",
                "# Execute Fetch\n",
                "try:\n",
                "    df_batting = fetch_data_by_decade(1970, 2025)\n",
                "    print(f\"\\nTotal Data Shape: {df_batting.shape}\")\n",
                "except Exception as e:\n",
                "    print(f\"Failed to load data: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Preprocessing & Aggregation\n",
                "FanGraphs data is season-level. We aggregate to **Career Totals**.\n",
                "\n",
                "Note: `batting_stats` returns columns like 'G', 'AB', 'H', 'HR', etc. We sum these up. Advanced stats like 'WAR' should also be summed (Career WAR)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if 'df_batting' in locals() and not df_batting.empty:\n",
                "    # Ensure numeric types for aggregation\n",
                "    cols_to_sum = ['G', 'AB', 'PA', 'H', '1B', '2B', '3B', 'HR', 'R', 'RBI', 'SB', 'BB', 'SO', 'WAR']\n",
                "    \n",
                "    # Clean data: Replace non-numeric values or handle strings if necessary\n",
                "    for col in cols_to_sum:\n",
                "        if col in df_batting.columns:\n",
                "            df_batting[col] = pd.to_numeric(df_batting[col], errors='coerce').fillna(0)\n",
                "\n",
                "    # Aggregate by Player ID (FanGraphs uses 'IDfg')\n",
                "    career_stats = df_batting.groupby('IDfg').agg({\n",
                "        'Name': 'first',  # Keep the name\n",
                "        **{col: 'sum' for col in cols_to_sum if col in df_batting.columns}\n",
                "    }).reset_index()\n",
                "\n",
                "    # Recalculate Rates (AVG, OBP, etc. - simplified)\n",
                "    # Note: Accurately recalculating OBP/SLG requires HBP/SF columns which we might need to check for.\n",
                "    # For simplicity, we'll stick to Sums and simple AVG.\n",
                "    career_stats['AVG'] = career_stats['H'] / career_stats['AB']\n",
                "    career_stats = career_stats.fillna(0)\n",
                "    \n",
                "    # Filter: Decent career length (> 2000 AB)\n",
                "    career_stats = career_stats[career_stats['AB'] > 2000]\n",
                "    \n",
                "    print(f\"Filtered Career Players: {len(career_stats)}\")\n",
                "    display(career_stats.head())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Creating Labels (HOF Status)\n",
                "Since we fetched raw stats without HOF labels, we'll engineer a **'Hall of Fame Standard'** label for training. \n",
                "\n",
                "Real-world data science often involves 'Silver Labels' when Gold Labels (actual HOF induction database) aren't essentially linkable. We'll mark a player as `is_hof` if they meet high statistical benchmarks widely accepted for induction."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if 'career_stats' in locals():\n",
                "    # Benchmarks for 'Automatic' or near-automatic induction\n",
                "    # 1. 60+ WAR (Borderline is 50-60, but 60 is safe for training positives)\n",
                "    # 2. 3000+ Hits\n",
                "    # 3. 500+ HR\n",
                "    career_stats['is_hof'] = (\n",
                "        (career_stats['WAR'] >= 60) |\n",
                "        (career_stats['H'] >= 3000) |\n",
                "        (career_stats['HR'] >= 500)\n",
                "    ).astype(int)\n",
                "\n",
                "    print(\"Derived HOF Inductees:\", career_stats['is_hof'].sum())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Model Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if 'career_stats' in locals():\n",
                "    features = ['G', 'AB', 'PA', 'R', 'H', 'HR', 'RBI', 'SB', 'BB', 'AVG', 'WAR']\n",
                "    X = career_stats[features]\n",
                "    y = career_stats['is_hof']\n",
                "\n",
                "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
                "\n",
                "    model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
                "    model.fit(X_train, y_train)\n",
                "    \n",
                "    print(\"Model trained.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if 'model' in locals():\n",
                "    y_pred = model.predict(X_test)\n",
                "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
                "    print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
                "\n",
                "    xgb.plot_importance(model, max_num_features=10)\n",
                "    plt.title(\"Key Stats for HOF Prediction\")\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Predictions for Recent Stars\n",
                "Let's see who the model likes from the full dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if 'model' in locals():\n",
                "    # Predict probabilities for everyone\n",
                "    career_stats['HOF_Prob'] = model.predict_proba(career_stats[features])[:, 1]\n",
                "    \n",
                "    # Show top candidates\n",
                "    top_candidates = career_stats.sort_values(by='HOF_Prob', ascending=False).head(20)\n",
                "    print(\"Top HOF Candidates (Model Probability):\")\n",
                "    display(top_candidates[['Name', 'WAR', 'H', 'HR', 'HOF_Prob']])"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}